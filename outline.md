# 제목: <슈판워> 맨땅에서 데이터 분석 시스템 만들어나가기

## 발표자 소개

### 내 소개

* blabla

### 팀 소개

* blabla

## 오늘 이야기할 내용

* 계기
* 최초의 삽질
    * DB 아키텍처를 보았다. 눈물..
    * EC2 머신 수배
    * S3에 올라간 전체 백업 복원
    * 합-체..에 며칠 걸림
* 합체해놓고 보니 이것도 보고 싶고 저것도 보고 싶고....
    * 그런데 도저히 이건 두 번은 안 된다
    * 보통 저는 기준이 삼연벙..아니 세 번인데 이건 두 번은 못하겠더라고요
* 아예 별개의 로그 분석 시스템을 만들자
    * 모든 로그를 다 가져가서 저장하고
    * 게임DB에서도 데이터를 읽어올 수 있는 시스템
    * 아는 게 도둑질이라고 Apache Spark 사용
    * 최초의 크롤링 스크립트
        * 우웩
        * mysql -> tsv -> (spark) -> parquet
            * Parquet
        * 사실 굳이 spark 없어도 되는 구조
        * 메타데이터는 SQLite에 (그리고 이건 영원히...ㅠㅠ)
* 그래도 데이터를 적재하는 데는 성공했다.
* 다음 단계: 데이터를 변환하자.
    * 얘도 간단한 스크립트를 짰다.
    * 최소한의 추상화
    * 메타데이터는 SQLite에 (만들어 두면 쓰게 됩니다)
    * 어차피 시간단위로 처리하긴 글렀고 일단위나 되면 다행이지!
    * 로그 테이블들에 대한 디펜던시를 정의하고 쭈루룩 돌림
    * 결과물을 다시 parquet으로 로컬 파일시스템에 저장
* (Short) Love with Spark shell
    * 쉘에서 이거저거 해 볼 수는 있었는데
    * 얘도 Spark Context임 (비쌈)
    * 데이터는 꽤 커서 CSV(엑셀)로 꺼내긴 그렇고
    * 분석가한테 스칼라를 가르칠 수는 없잖아요...
* How to explore?
    * Data exploration은 어려움
    * (적당히) 큰 데이터를 어떻게 효과적으로 다룰까
    * Disclaimer: I worked at Palantir
        * 데이터 철학
    * Introduction to Business Intelligence
    * 데이터를 분석하기에 그나마 가장 좋은 도구
* 다음 단계: 데이터를 불러들이자.
    * ETL 파이프라인의 마지막 단계
    * 그냥 parquet 파일을 불러들일 수 있게 하려고 했는데, Spark SQL connector 어려움 + Hive 세팅에 대한 부담
    * 로컬에 있던 mysqld에 결과를 매일 쏟아부었다 (최근 하루치만)
    * 이것도 batch job의 일부로 구현
* Tidbits
    * Slack webhook
    * Feedback loop: Data -> Exploration -> Information ==> Automation
* Unsustainable System
    * 이렇게 쓰라고 만든 도구들이 아니지만 그냥 돌아가고 있는 것 뿐이예요
    * Spark: 컴퓨팅 클러스터 위에서 분산 파일시스템 위에서 쓰라고 만든 도구예요
    * MySQL: 용도에 맞게 쓰는지 의심스러워요
    * SQLite: 터지면 어쩌려고요
    * 로컬 파일시스템: .......
    * 뭔가 맨날 OOM나고.. 죽고... 엉엉 (원인은 나중에 설명하겠습니다)
    * 그래도 돌아는 간다. 안 돌아가는 것보다는 낫지
* Sustainable (Ideal) System?
    * (EMR + )EMRFS
    * Hive, remove usage of mysql
    * Remove usage of sqlite
    * User-centric analysis
    * Realtimeness
    * ...
* 돌아봤을 때 왜 이게 가능했는가?
    * 모래밭에 무언가를 지을 순 없다. 단단한 맨땅(인프라)이 필요하다.
    * 무에서 유를 창조할 순 없다. 기존에 믿을 수 있는 데이터를 충분히 쌓고 있었는지가 중요하다.
    * 정형 로그
* 좀 더 잘할 수 있었던 건 뭔가?
    * '업적'은 훌륭한 정보이다. 특정 업적들의 보유 여부는 이미 그 유저에 대해 여러가지 분석을 돌린 결과물이나 다름없다. 안그래도 업적 구현하는 게 쉽지 않은데, 이것 보유 획득 시각만 잘 기록하고 있어도 많은 정보를 얻을 수 있다.
    * unique해야 할 건 충분히 unique하게 만드는 편이 좋다. 유저 ID를 리전으로 분산하였더라도 시작 값을 다르게 잡으면 unique하게 만들 수 있다. 일별로 쪼개는 1회성 키의 길이도 충분히 길게 만들지 않으면 충돌이 1건이라도 일어날 확률이 높다 (birthday paradox)
    * 잘못 쌓은 로그들
* Spark를 쓴 건 어땠나?
    * Spark SQL은 엄청나게 좋은 도구이다. Parquet도 좋은 도구이지만, nested structure는 좋지 않을 수 있다.
    * 너무 긴 RDD 체인은 좋지 않다. 체크포인팅을 하거나 해야 한다.
    * 디버깅할 때 가장 중요한 건 Task가 죽었는지, Driver가 죽었는지, Executor가 죽었는지 구분하는 것이다.
    * Dynamic types in Strong typed language(Scala): Ouch
    * 로컬 파일시스템을 저렇게 혹사시키면 inode full + globbing OOM + ... 문제가 여러가지 생긴다
    * Databricks CSV type inference 주의
* BI는?
    * 시각화는 중요하고, 그걸 프로그래머가 아닌 사람들도 할 수 있느냐는 더더욱 중요하다.
* Hint of Ideal architecture

## 어쩌다 이런 일을 하게 되었나?

* 슈판워를 런칭했습니다. 짜잔~
* 런칭하고 보니 이런저런 분석이 하고 싶었습니다. 어뷰징 탐지라던가요.
* 그래서 저희 팀에서 일손을 거들러 갔습니다.


* 스토리라인
    * 지금까지 한 일  
        * 슈판워를 런칭했다.
        * 런칭하고 보니 유저들에 대한 이런저런 분석이 하고 싶었다.
        * 말로 하면 별로 복잡하지 않지만, 시스템 아키텍처가 이런 상황이었다.
        * 결국 가장 먼저 해 볼 수 있는 건 모든 데이터베이스를 복원하는 것이었다.
        * 노동력도 많이 사용했고, 어마어마하게 시간이 오래 걸렸다. 이미 유통기한이 지난 데이터가 되어버렸다.
        * 이렇게 된 거 급하게 일단 시스템을 구축해 보자! AWS 계정은 없지만 일단 급하게라도!
        * 아는 게 도둑질이라고 Apache Spark를 쓰자. Spark 소개. Spark SQL 소개.
        * Spark에 가장 잘 어울리는 데이터 포맷 Parquet. 간략히 소개
        * 그래서 일단 로그를 다 긁어 오자. 계속 긁어오는 코드를 만들었다. 얼마나 급했냐하면 파이썬 코드를 뱉어내는 루비 코드를 만들었다. 후회된다.
        * 그리고 그걸 바탕으로 일별로 어그리게이션을 시작했다. Job을 추상화하고, 어느 어느 로그 테이블의 일별 조회가 완료되어야 돌아갈 수 있는지 간단한 스크립트를 구현했다. 이 모든 메타데이터는 SQLite에 그냥 쌓았다.
        * 잘 돌아가서 Spark shell에선 조회할 수 있었다.
        * 그런데 이 데이터 분석을 매번 필요로 하는 데이터를 수동으로 전달해 주기도 어렵고, 데이터 사이즈도 커서 받는 쪽에서도 엑셀로 처리할 수 있는 한계를 넘어갔다.
        * 데이터 분석용 도구로 Tableau를 사용하기로 결정했다. 시각화는 중요하고, 그걸 프로그래머가 아닌 사람들도 할 수 있느냐는 더더욱 중요하기 때문이다. 어느 정도로 큰 데이터 사이즈까지는 견딜 수 있었다.
        * 그리고 Tableau와 Spark를 바로 붙일 수는 없어서 (Hive 설정을 하지 못하고 있었음) MySQL로 데이터를 매일 뱉어내기 시작했다. 로그에 없는 정보들도 포함해 뱉어내기 시작했다.
        * 이렇게 만들어놓고 보니 이게 ETL (Extract, Transform, Load) 파이프라인이었다.
        * Slack webhook으로 일간 분석이 끝나면 알려주도록 했다. 
        * 분석가들은 Tableau에서 이것저것 분석을 수행할 수 있었다.
        * 그 과정에서 발견한 새로운 인사이트나, 더 필요한 데이터나, Tableau에서 처리하기 힘든 내용들을 요청받아 Job들을 추가적으로 구현해주기 시작했다. 
    * 미래편 (NDC 발표 전까지 할 일)
        * 하지만 건물을 지어 놓은 인프라가 빈약해 지반침하급 사건이 자주 일어났다.
        * 또 새로운 분석 지표(잡)을 추가할 때 빨리 전부 처리하고 싶지만 그러기엔 일별로 해야 하는 일이 너무 많아 컴퓨팅 성능이 부족했다.
        * Spark를 사용한다는 건 컴퓨팅 자원을 유연하게 사용할 수 있다는 건데 그 장점이 많이 퇴색된 상태였다.
        * 급한 일들이 지나간 뒤 단단한 지반 위에 건물을 새로 지었다.
        * AWS 위에 EMR 클러스터를 새로 세팅하고, 데이터는 EMRFS로 저장했다.
        * MySQL 인스턴스와 ETL 관리자(드라이버) 역할을 할 마스터 노드를 두었다.
        * Hive를 세팅해서 테이블들이 잘 관리되도록 하고, MySQL로 내보내지 않아도 Hive (또는 Spark SQL Connector)를 통해 바로 받아올 수 있게 되었다.
* 결론과 Random Insights
    * 모래밭에 무언가를 지을 순 없다. 단단한 맨땅(인프라)이 필요하다.
    * 무에서 유를 창조할 순 없다. 기존에 믿을 수 있는 데이터를 충분히 쌓고 있었는지가 중요하다.
    * 시각화는 중요하고, 그걸 프로그래머가 아닌 사람들도 할 수 있느냐는 더더욱 중요하다.
    * '업적'은 훌륭한 정보이다. 특정 업적들의 보유 여부는 이미 그 유저에 대해 여러가지 분석을 돌린 결과물이나 다름없다. 안그래도 업적 구현하는 게 쉽지 않은데, 이것 보유 획득 시각만 잘 기록하고 있어도 많은 정보를 얻을 수 있다.
    * unique해야 할 건 충분히 unique하게 만드는 편이 좋다. 유저 ID를 리전으로 분산하였더라도 시작 값을 다르게 잡으면 unique하게 만들 수 있다. 일별로 쪼개는 1회성 키의 길이도 충분히 길게 만들지 않으면 충돌이 1건이라도 일어날 확률이 높다 (birthday paradox)
    * Spark
        * Spark SQL은 엄청나게 좋은 도구이다. Parquet도 좋은 도구이지만, nested structure는 좋지 않을 수 있다.
        * 너무 긴 RDD 체인은 좋지 않다. 체크포인팅을 하거나 해야 한다.
        * 디버깅할 때 가장 중요한 건 Task가 죽었는지, Driver가 죽었는지, Executor가 죽었는지 구분하는 것이다.
        * OOM은 피할 수 없는 숙명이다. 메모리 할당 메커니즘은 쉽게 이해하긴 어렵다.
